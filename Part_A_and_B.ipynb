{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Install and import packages, mount **drive**","metadata":{"id":"qyaZotPCyoQ8"}},{"cell_type":"code","source":"pip install simpletransformers","metadata":{"id":"ECODTAGzrCRF","outputId":"2bc0e248-631b-4a8f-f705-960f66c36686","execution":{"iopub.status.busy":"2022-05-18T10:41:19.319907Z","iopub.execute_input":"2022-05-18T10:41:19.320258Z","iopub.status.idle":"2022-05-18T10:41:27.548413Z","shell.execute_reply.started":"2022-05-18T10:41:19.320156Z","shell.execute_reply":"2022-05-18T10:41:27.547633Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"pip install transformers","metadata":{"execution":{"iopub.status.busy":"2022-05-18T10:41:27.550123Z","iopub.execute_input":"2022-05-18T10:41:27.550388Z","iopub.status.idle":"2022-05-18T10:41:34.816484Z","shell.execute_reply.started":"2022-05-18T10:41:27.550350Z","shell.execute_reply":"2022-05-18T10:41:34.815713Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport logging\nimport random\nrandom.seed(0)\n\nfrom simpletransformers.classification import ClassificationModel, ClassificationArgs\n\n#from google.colab import drive\n#drive.mount('/content/gdrive/')","metadata":{"id":"UngOggc7o5IO","execution":{"iopub.status.busy":"2022-05-18T10:41:34.820469Z","iopub.execute_input":"2022-05-18T10:41:34.820750Z","iopub.status.idle":"2022-05-18T10:41:39.314247Z","shell.execute_reply.started":"2022-05-18T10:41:34.820714Z","shell.execute_reply":"2022-05-18T10:41:39.313375Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# 0. Preparation and definitions","metadata":{"id":"_Xi138SaQGG4"}},{"cell_type":"markdown","source":"**Import the data**","metadata":{"id":"WPGM4tFuyu7K"}},{"cell_type":"code","source":"# Import the data\ntrain = pd.read_csv('../input/olid-data/olid-train.csv')\ntest = pd.read_csv('../input/olid-data/olid-test.csv')","metadata":{"id":"odgdhhJHpo0D","execution":{"iopub.status.busy":"2022-05-18T10:41:39.317149Z","iopub.execute_input":"2022-05-18T10:41:39.317413Z","iopub.status.idle":"2022-05-18T10:41:39.394020Z","shell.execute_reply.started":"2022-05-18T10:41:39.317378Z","shell.execute_reply":"2022-05-18T10:41:39.393176Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"**Making the evaluation call; precision, recall and F1**","metadata":{"id":"Ee1hGi8-zfYw"}},{"cell_type":"code","source":"def evaluation(df, freq_0, freq_1):\n    df['TP'] = (df['labels'] == 1) & (df['labels'] == df['predictions'])\n    df['FN'] = (df['labels'] == 1) & (df['labels'] != df['predictions'])\n    df['FP'] = (df['labels'] == 0) & (df['labels'] != df['predictions'])\n    df['TN'] = (df['labels'] == 0) & (df['labels'] == df['predictions'])\n\n    precision_1 = sum(df['TP']) / (sum(df['TP']) + sum(df['FP'])) if (sum(df['TP']) + sum(df['FP']) > 0) else 0\n    precision_0 = sum(df['TN']) / (sum(df['FN']) + sum(df['TN'])) if (sum(df['FN']) + sum(df['TN']) > 0) else 0\n    precision_avg = np.mean([precision_1, precision_0])\n    precision_wavg = freq_0 * precision_0 + freq_1 * precision_1\n\n    recall_1 = sum(df['TP']) / (sum(df['TP']) + sum(df['FN'])) if (sum(df['TP']) + sum(df['FN']) > 0) else 0\n    recall_0 = sum(df['TN']) / (sum(df['FP']) + sum(df['TN'])) if (sum(df['TP']) + sum(df['FN']) > 0) else 0\n    recall_avg = np.mean([recall_1, recall_0])\n    recall_wavg = freq_0 * recall_0 + freq_1 * recall_1\n\n    F1_1 = 2 * (precision_1 * recall_1) / (precision_1 + recall_1) if (precision_1 + recall_1 > 0) else 0\n    F1_0 = 2 * (precision_0 * recall_0) / (precision_0 + recall_0) if (precision_0 + recall_0 > 0) else 0\n    F1_avg = np.mean([F1_1, F1_0])\n    F1_wavg = freq_0 * F1_0 + freq_1 * F1_1\n\n    print('metric, class_1, class_0, avg, wavg')\n    print(\"precision: \", precision_1, precision_0, precision_avg, precision_wavg)\n    print(\"recall: \", recall_1, recall_0, recall_avg, recall_wavg)\n    print(\"F1: \", F1_1, F1_0, F1_avg, F1_wavg)","metadata":{"id":"1VsXpOlwyXmX","execution":{"iopub.status.busy":"2022-05-18T10:41:39.397360Z","iopub.execute_input":"2022-05-18T10:41:39.398114Z","iopub.status.idle":"2022-05-18T10:41:39.412840Z","shell.execute_reply.started":"2022-05-18T10:41:39.398072Z","shell.execute_reply":"2022-05-18T10:41:39.412078Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# 1. Class distributions (1 point)","metadata":{"id":"rc9IUbpm2tFu"}},{"cell_type":"code","source":"# 1. Class distributions (1 point)\r\nprint(train['labels'].value_counts())\r\nprint(train['labels'].value_counts(normalize=True))\r\nfreq_0 = train['labels'].value_counts(normalize=True).iloc[0]\r\nfreq_1 = train['labels'].value_counts(normalize=True).iloc[1]\r\nprint(train[train['labels'] == 0].iloc[0]['text'])\r\nprint(train[train['labels'] == 1].iloc[0]['text'])","metadata":{"id":"JuUx7spdyfcA","outputId":"b9c5c141-96c5-4182-9710-2b1b595bddbb","execution":{"iopub.status.busy":"2022-05-18T10:41:39.414032Z","iopub.execute_input":"2022-05-18T10:41:39.414329Z","iopub.status.idle":"2022-05-18T10:41:39.455862Z","shell.execute_reply.started":"2022-05-18T10:41:39.414286Z","shell.execute_reply":"2022-05-18T10:41:39.455121Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# 2.\tBaselines (1 point) ","metadata":{"id":"Yb29wdsc2YJo"}},{"cell_type":"code","source":"# Random\ndf_random = test[['text', 'labels']]\npredictions = []\nfor i in range(len(df_random)):\n    if random.random() > 0.5:\n        predictions.append(1)\n    else:\n        predictions.append(0)\ndf_random['predictions'] = predictions\nevaluation(df_random, freq_0, freq_1)","metadata":{"id":"_Oc7Fph02Xpi","outputId":"301c1895-d0a3-4687-f6fc-0dcf0f47ca4d","execution":{"iopub.status.busy":"2022-05-18T10:41:39.457245Z","iopub.execute_input":"2022-05-18T10:41:39.457590Z","iopub.status.idle":"2022-05-18T10:41:39.480547Z","shell.execute_reply.started":"2022-05-18T10:41:39.457538Z","shell.execute_reply":"2022-05-18T10:41:39.479649Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Majority\ndf = train[['text', 'labels']]\nmajority_class = df['labels'].value_counts().idxmax()\ndf_majority = test[['text', 'labels']]\npredictions = [majority_class] * len(df_majority)\ndf_majority['predictions'] = predictions\nevaluation(df_majority, freq_0, freq_1)","metadata":{"id":"mb_HakG4Jevt","outputId":"8d2f28e2-4d40-40d6-a8d6-19633b9d8ef7","execution":{"iopub.status.busy":"2022-05-18T10:41:39.481892Z","iopub.execute_input":"2022-05-18T10:41:39.482319Z","iopub.status.idle":"2022-05-18T10:41:39.505676Z","shell.execute_reply.started":"2022-05-18T10:41:39.482281Z","shell.execute_reply":"2022-05-18T10:41:39.504947Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# 3.\tClassification by fine-tuning BERT (2.5 points)","metadata":{"id":"PRhE8LZKyzdd"}},{"cell_type":"code","source":"# # Preparing short train data\n# train_df_trim = train.iloc[:1000,:][['text','labels']]\n# # Preparing short eval data\n# eval_df_trim = train.iloc[1000:1200,:][['text','labels']]\n# # Preparing short test data\n# test_df_trim = test.iloc[:100,:][['text','labels']]\n# test_list_trim = test_df_trim['text'].values.tolist()[:100]\n\n# logging.basicConfig(level=logging.INFO)\n# transformers_logger = logging.getLogger(\"transformers\")\n# transformers_logger.setLevel(logging.WARNING)\n\n# cuda_available = torch.cuda.is_available()\n# model = ClassificationModel(\n#     \"bert\", \"bert-base-cased\", use_cuda=cuda_available\n# )\n\n# # Train the model\n# model.train_model(train_df_trim)\n\n# # Evaluate the model\n# result, model_outputs, wrong_predictions = model.eval_model(eval_df_trim)\n\n# # Make predictions with the model\n# predictions, raw_outputs = model.predict(test_list_trim)\n\n# # Attach predictions to test df for evaluation\n# test_df_trim['predictions'] = predictions","metadata":{"id":"F2jaai1BsBWi","execution":{"iopub.status.busy":"2022-05-18T10:41:39.506951Z","iopub.execute_input":"2022-05-18T10:41:39.507509Z","iopub.status.idle":"2022-05-18T10:41:39.512409Z","shell.execute_reply.started":"2022-05-18T10:41:39.507459Z","shell.execute_reply":"2022-05-18T10:41:39.511470Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Preparing train data\ntrain_df = train.iloc[:10000,:][['text','labels']] #+/- 80% of the training set\n# Preparing eval data\neval_df = train.iloc[10000:,:][['text','labels']] #+/- 20% of the training set\n# Preparing test data\ntest_df = test[['text','labels']]\ntest_list = test_df['text'].values.tolist()\n\nprint(len(train_df))\n# print(train_df.head())\n# print(len(eval_df))\n# print(eval_df.head())\n# print(len(test_list))\n# print(test_list[:2])\n\nlogging.basicConfig(level=logging.INFO)\ntransformers_logger = logging.getLogger(\"transformers\")\ntransformers_logger.setLevel(logging.WARNING)\n\nmodel_args = ClassificationArgs()\nmodel_args.overwrite_output_dir = True\n\ncuda_available = torch.cuda.is_available()\nprint(cuda_available)\nmodel = ClassificationModel(\n    \"bert\", \"bert-base-cased\", use_cuda=cuda_available, args=model_args\n)\n\n# Train the model\nmodel.train_model(train_df)\n\n# Evaluate the model\nresult, model_outputs, wrong_predictions = model.eval_model(eval_df)\n\n# Make predictions with the model\npredictions, raw_outputs = model.predict(test_list)","metadata":{"id":"yAyxR0JczQOh","outputId":"fe12976d-83b0-444c-fd37-b20ae64cb6fa","execution":{"iopub.status.busy":"2022-05-18T10:41:39.514044Z","iopub.execute_input":"2022-05-18T10:41:39.514444Z","iopub.status.idle":"2022-05-18T10:46:07.519825Z","shell.execute_reply.started":"2022-05-18T10:41:39.514275Z","shell.execute_reply":"2022-05-18T10:46:07.518857Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Attach predictions to test df for evaluation\ntest_df['predictions'] = predictions\nprint(test_df)\n\n# Calculate evaluation metrics\nevaluation(test_df, freq_0, freq_1)\n\nprint(sum(test_df['predictions'] == test_df['labels']))\nprint(len(test_df['predictions']))","metadata":{"id":"DbZzXE8U2FPo","execution":{"iopub.status.busy":"2022-05-18T10:46:07.521893Z","iopub.execute_input":"2022-05-18T10:46:07.522237Z","iopub.status.idle":"2022-05-18T10:46:07.555040Z","shell.execute_reply.started":"2022-05-18T10:46:07.522195Z","shell.execute_reply":"2022-05-18T10:46:07.554282Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Confusion matrix elements\nprint('TP: ', sum(test_df['TP']))\nprint('FN: ', sum(test_df['FN']))\nprint('FP: ', sum(test_df['FP']))\nprint('TN: ', sum(test_df['TN']))","metadata":{"id":"jGPvnCLIBdnY","execution":{"iopub.status.busy":"2022-05-18T10:46:07.556422Z","iopub.execute_input":"2022-05-18T10:46:07.556751Z","iopub.status.idle":"2022-05-18T10:46:07.563721Z","shell.execute_reply.started":"2022-05-18T10:46:07.556714Z","shell.execute_reply":"2022-05-18T10:46:07.562945Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# 4.\tInspect the tokenization of the OLIDv1 training set using the BERT’s tokenizer (2.5 points)","metadata":{"id":"jYXtfF0vNryU"}},{"cell_type":"code","source":"train_text = train['text'].values.tolist()\n#print(train_text)\ntrain_tokens = []\nfor i in range(len(train_text)):\n  tokens = model.tokenizer.tokenize(train_text[i])\n  train_tokens.extend(tokens)","metadata":{"id":"yxdTma0INqbZ","execution":{"iopub.status.busy":"2022-05-18T10:46:07.568739Z","iopub.execute_input":"2022-05-18T10:46:07.569320Z","iopub.status.idle":"2022-05-18T10:46:09.673918Z","shell.execute_reply.started":"2022-05-18T10:46:07.569284Z","shell.execute_reply":"2022-05-18T10:46:09.671064Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# number of tokens\nprint(len(train_tokens))\n\n# number of token split into subwords\ntrain_tokens_str = ' '.join(train_tokens)\nprint(train_tokens_str.count('##'))\ntrain_tokens_str[:10000]","metadata":{"id":"x8hq39S4IMwu","execution":{"iopub.status.busy":"2022-05-18T10:46:09.678922Z","iopub.execute_input":"2022-05-18T10:46:09.681151Z","iopub.status.idle":"2022-05-18T10:46:09.745319Z","shell.execute_reply.started":"2022-05-18T10:46:09.681105Z","shell.execute_reply":"2022-05-18T10:46:09.743291Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# How long (in characters) is the longest subword in the BERT’s vocabulary? (0.5 points)\nprint(max(list(model.tokenizer.vocab.keys()), key=len))\nprint(len(max(list(model.tokenizer.vocab.keys()), key=len)))","metadata":{"id":"d8GliiofKYrB","execution":{"iopub.status.busy":"2022-05-18T10:46:09.746724Z","iopub.execute_input":"2022-05-18T10:46:09.747106Z","iopub.status.idle":"2022-05-18T10:46:09.808522Z","shell.execute_reply.started":"2022-05-18T10:46:09.747067Z","shell.execute_reply":"2022-05-18T10:46:09.807783Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"!pip install checklist\n!pip install tabulate","metadata":{"id":"UCvJT5jP297S","execution":{"iopub.status.busy":"2022-05-18T10:46:09.813128Z","iopub.execute_input":"2022-05-18T10:46:09.814717Z","iopub.status.idle":"2022-05-18T10:46:25.593281Z","shell.execute_reply.started":"2022-05-18T10:46:09.814675Z","shell.execute_reply":"2022-05-18T10:46:25.592431Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport checklist\nfrom checklist.perturb import Perturb\n\n# reseed\nnp.random.seed(42)\n\n# text\nsubset_test = pd.read_csv('../input/olid-data/olid-subset-diagnostic-tests.csv')\ntext = subset_test['text'].tolist()","metadata":{"id":"1bHqRXL86MYl","execution":{"iopub.status.busy":"2022-05-18T10:47:14.906194Z","iopub.execute_input":"2022-05-18T10:47:14.906482Z","iopub.status.idle":"2022-05-18T10:47:14.916795Z","shell.execute_reply.started":"2022-05-18T10:47:14.906445Z","shell.execute_reply":"2022-05-18T10:47:14.915939Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# overload existing method from checklist to add more than 1 typo\ndef add_typos(string, typos=5):\n        \"\"\"Perturbation functions, swaps random characters with their neighbors\n        Parameters\n        ----------\n        string : str\n            input string\n        typos : int\n            number of typos to add\n        Returns\n        -------\n        list(string)\n            perturbed strings\n        \"\"\"\n        string = list(string)\n        swaps = np.random.choice(len(string) - 1, typos)\n        for swap in swaps:\n            tmp = string[swap]\n            string[swap] = string[swap + 1]\n            string[swap + 1] = tmp\n        return ''.join(string)","metadata":{"id":"hkOUFttSmVG7","execution":{"iopub.status.busy":"2022-05-18T10:47:15.627129Z","iopub.execute_input":"2022-05-18T10:47:15.629729Z","iopub.status.idle":"2022-05-18T10:47:15.638299Z","shell.execute_reply.started":"2022-05-18T10:47:15.629684Z","shell.execute_reply":"2022-05-18T10:47:15.637514Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# add_typos\ntypos = Perturb.perturb(text, add_typos)\n#print(typos.data)\n\n# plug back into pandas\nnew_text = []\nfor t in typos.data:\n  new_text.append(t[1])\nsubset_test['text_typos'] = new_text\nprint(subset_test)","metadata":{"id":"W0_IIWKXAAyD","execution":{"iopub.status.busy":"2022-05-18T10:47:44.963387Z","iopub.execute_input":"2022-05-18T10:47:44.963709Z","iopub.status.idle":"2022-05-18T10:47:44.984895Z","shell.execute_reply.started":"2022-05-18T10:47:44.963674Z","shell.execute_reply":"2022-05-18T10:47:44.983944Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# Preparing test data\nsubset_test_df = subset_test[['text','labels']]\nsubset_test_list = subset_test_df['text'].values.tolist()\n\n# Preparing typo test data\nsubset_typos_test_df = subset_test[['text_typos','labels']]\nsubset_typos_test_list = subset_typos_test_df['text_typos'].values.tolist()\n\n# using model trained previously\n\nlogging.basicConfig(level=logging.INFO)\ntransformers_logger = logging.getLogger(\"transformers\")\ntransformers_logger.setLevel(logging.WARNING)\n\n# Make predictions with the model\nsubset_predictions, subset_raw_outputs = model.predict(subset_test_list)\n#print(predictions, raw_outputs)\n\n# Make predictions with the model on typos dataset\nsubset_typos_predictions, subset_typos_raw_outputs = model.predict(subset_typos_test_list)\n#print(predictions, raw_outputs)\n\n# Attach predictions to test df for evaluation\nsubset_test_df['predictions'] = subset_predictions\n\n# Attach predictions to typo test df for evaluation\nsubset_typos_test_df['predictions'] = subset_typos_predictions\n\n# get class distributions for getting weighted F1 score later\nsubset_freq_0 = subset_test_df['labels'].value_counts(normalize=True).iloc[0]\nsubset_freq_1 = subset_test_df['labels'].value_counts(normalize=True).iloc[1]\n# note that class distributions are equal in this subset so weighted and macro\n# F1 scores will be the same\n\n# Calculate evaluation metrics\nevaluation(subset_test_df, subset_freq_0, subset_freq_1)\nprint('Correctly identified messages in data: ' + str(sum(subset_test_df['predictions'] == subset_test_df['labels'])))\nprint('Total number of messages: ' + str(len(subset_test_df['predictions'])))\n\n# Calculate evaluation metrics for typos\nevaluation(subset_typos_test_df, subset_freq_0, subset_freq_1)\nprint('Correctly identified messages in typo data: ' + str(sum(subset_typos_test_df['predictions'] == subset_typos_test_df['labels'])))\nprint('Total number of messages: ' + str(len(subset_typos_test_df['predictions'])))","metadata":{"id":"lF47_HdaMuz0","execution":{"iopub.status.busy":"2022-05-18T10:47:45.758438Z","iopub.execute_input":"2022-05-18T10:47:45.759258Z","iopub.status.idle":"2022-05-18T10:47:47.523958Z","shell.execute_reply.started":"2022-05-18T10:47:45.759220Z","shell.execute_reply":"2022-05-18T10:47:47.519583Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# save dataframes to html\nsubset_test_df.to_html('/kaggle/working/subset_test_df.html')\nsubset_typos_test_df.to_html('/kaggle/working/subset_typos_test_df.html')\n\n# read dataframes from html\n#subset_test_df = pd.read_html('../input/olid-subset-results/subset_test_df.html', index_col=0)[0]\n#subset_typos_test_df = pd.read_html('../input/olid-subset-results/subset_typos_test_df.html', index_col=0)[0]","metadata":{"execution":{"iopub.status.busy":"2022-05-18T10:52:59.254299Z","iopub.execute_input":"2022-05-18T10:52:59.254725Z","iopub.status.idle":"2022-05-18T10:52:59.300395Z","shell.execute_reply.started":"2022-05-18T10:52:59.254690Z","shell.execute_reply":"2022-05-18T10:52:59.299606Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"print(subset_typos_test_df['text_typos'][4])\nprint(subset_typos_test_df['text_typos'][16])\nprint(subset_typos_test_df['text_typos'][29])","metadata":{"id":"h2lUSBmBeGy8","execution":{"iopub.status.busy":"2022-05-18T11:01:44.917230Z","iopub.execute_input":"2022-05-18T11:01:44.917685Z","iopub.status.idle":"2022-05-18T11:01:44.923771Z","shell.execute_reply.started":"2022-05-18T11:01:44.917651Z","shell.execute_reply":"2022-05-18T11:01:44.923042Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"pip install spacy","metadata":{"execution":{"iopub.status.busy":"2022-05-18T11:01:47.869885Z","iopub.execute_input":"2022-05-18T11:01:47.870524Z","iopub.status.idle":"2022-05-18T11:01:55.017081Z","shell.execute_reply.started":"2022-05-18T11:01:47.870484Z","shell.execute_reply":"2022-05-18T11:01:55.016215Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"import spacy\nnlp = spacy.load('en_core_web_sm')\npdata = list(nlp.pipe(text))\n\n#print(pdata)\n# reload data\nsubset_test = pd.read_csv('../input/olid-data/olid-subset-diagnostic-tests.csv')\ntext = subset_test['text'].tolist()","metadata":{"execution":{"iopub.status.busy":"2022-05-18T11:01:55.238064Z","iopub.execute_input":"2022-05-18T11:01:55.238436Z","iopub.status.idle":"2022-05-18T11:01:57.121367Z","shell.execute_reply.started":"2022-05-18T11:01:55.238396Z","shell.execute_reply":"2022-05-18T11:01:57.120644Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# add_negations\nnegations = Perturb.perturb(pdata, Perturb.add_negation)\n#print(negations.data)\n#print(len(subset_test))\n#print(len(negations.data))\n\nids = []\ntext = []\ntext_negations = []\nlabels = []\n\n# build negations dataframe from scratch since the method uses spacy\nfor n in negations.data:\n    for index, row in subset_test.iterrows():\n        if n[0] == row['text']:\n            ids.append(row['id'])\n            text.append(n[0])            \n            labels.append(row['labels'])\n            text_negations.append(n[1])\n\n# initialize data of lists.\nneg_data = {'id': ids, 'text': text,  'labels': labels, 'text_negations': text_negations}\nsubset_test = pd.DataFrame(neg_data)\nprint(subset_test)","metadata":{"id":"K0ID2jmbLtp6","execution":{"iopub.status.busy":"2022-05-18T11:02:02.347854Z","iopub.execute_input":"2022-05-18T11:02:02.348157Z","iopub.status.idle":"2022-05-18T11:02:03.035790Z","shell.execute_reply.started":"2022-05-18T11:02:02.348115Z","shell.execute_reply":"2022-05-18T11:02:03.034912Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# Preparing test data\nsubset_test_df = subset_test[['text','labels']]\nsubset_test_list = subset_test_df['text'].values.tolist()\n\n# Preparing negations test data\nsubset_negs_test_df = subset_test[['text_negations','labels']]\nsubset_negs_test_list = subset_negs_test_df['text_negations'].values.tolist()\n\n# using model trained previously\n\nlogging.basicConfig(level=logging.INFO)\ntransformers_logger = logging.getLogger(\"transformers\")\ntransformers_logger.setLevel(logging.WARNING)\n\n# Make predictions with the model\nsubset_predictions, subset_raw_outputs = model.predict(subset_test_list)\n#print(predictions, raw_outputs)\n\n# Make predictions with the model on negations dataset\nsubset_negs_predictions, subset_negs_raw_outputs = model.predict(subset_negs_test_list)\n#print(predictions, raw_outputs)\n\n# Attach predictions to test df for evaluation\nsubset_test_df['predictions'] = subset_predictions\n\n# Attach predictions to negations test df for evaluation\nsubset_negs_test_df['predictions'] = subset_negs_predictions\n\n# get class distributions for getting weighted F1 score later\nsubset_freq_0 = subset_test_df['labels'].value_counts(normalize=True).iloc[0]\nsubset_freq_1 = subset_test_df['labels'].value_counts(normalize=True).iloc[1]\n# note that class distributions are equal in this subset so weighted and macro\n# F1 scores will be the same\n\n# Calculate evaluation metrics\nevaluation(subset_test_df, subset_freq_0, subset_freq_1)\nprint('Correctly identified messages in data: ' + str(sum(subset_test_df['predictions'] == subset_test_df['labels'])))\nprint('Total number of messages: ' + str(len(subset_test_df['predictions'])))\n\n# Calculate evaluation metrics for negations\nevaluation(subset_negs_test_df, subset_freq_0, subset_freq_1)\nprint('Correctly identified messages in negations data: ' + str(sum(subset_negs_test_df['predictions'] == subset_negs_test_df['labels'])))\nprint('Total number of messages: ' + str(len(subset_negs_test_df['predictions'])))","metadata":{"execution":{"iopub.status.busy":"2022-05-18T11:02:25.019552Z","iopub.execute_input":"2022-05-18T11:02:25.020284Z","iopub.status.idle":"2022-05-18T11:02:26.730134Z","shell.execute_reply.started":"2022-05-18T11:02:25.020249Z","shell.execute_reply":"2022-05-18T11:02:26.729270Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"# save dataframes to html\n#subset_test_df.to_html('/kaggle/working/subset_test_df.html')\nsubset_negs_test_df.to_html('/kaggle/working/subset_negs_test_df.html')\n\n# read dataframes from html\n#subset_test_df = pd.read_html('../input/olid-subset-results/subset_test_df.html', index_col=0)[0]\n#subset_typos_test_df = pd.read_html('../input/olid-subset-results/subset_typos_test_df.html', index_col=0)[0]","metadata":{"execution":{"iopub.status.busy":"2022-05-18T11:02:43.697968Z","iopub.execute_input":"2022-05-18T11:02:43.698260Z","iopub.status.idle":"2022-05-18T11:02:43.721259Z","shell.execute_reply.started":"2022-05-18T11:02:43.698229Z","shell.execute_reply":"2022-05-18T11:02:43.720353Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"# question 7\nfrom checklist.editor import Editor\n\neditor = Editor()","metadata":{"execution":{"iopub.status.busy":"2022-05-18T11:33:09.226424Z","iopub.execute_input":"2022-05-18T11:33:09.227098Z","iopub.status.idle":"2022-05-18T11:33:09.553403Z","shell.execute_reply.started":"2022-05-18T11:33:09.227061Z","shell.execute_reply":"2022-05-18T11:33:09.552636Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"# getting masked language model suggestions\nhate = editor.suggest('I hate {mask}.')[:30]\nnot_hate = editor.suggest('I don\\'t hate {mask}.')[:30]\n\nhate_mask = []\nfor i in hate:\n    hate_mask.append('I hate ' + i)\n#print(hate_mask)\n\nnot_hate_mask = []\nfor i in hate:\n    not_hate_mask.append('I don\\'t hate ' + i)\n#print(not_hate_mask)\n\n# using the built-in lexicon, we also explore more specific mask subjects\nret1 = editor.template('I hate {nationality}.')\nret2 = editor.template('I don\\'t hate {nationality}.')\nret3 = editor.template('I hate {religion}.')\nret4 = editor.template('I don\\'t hate {religion}.')\n\nhate_nationalities = list(np.random.choice(ret1.data, 10))\nnationalities = list(np.random.choice(ret2.data, 10))\nhate_religions = list(np.random.choice(ret3.data, 10))\nreligions = list(np.random.choice(ret4.data, 10))\n\n# build dataset\nhate_list = hate_mask + not_hate_mask + hate_nationalities + nationalities + hate_religions + religions\nhate_df = pd.DataFrame({'text': hate_list})\nprint(hate_df)","metadata":{"execution":{"iopub.status.busy":"2022-05-18T12:27:25.106511Z","iopub.execute_input":"2022-05-18T12:27:25.107307Z","iopub.status.idle":"2022-05-18T12:27:25.169035Z","shell.execute_reply.started":"2022-05-18T12:27:25.107273Z","shell.execute_reply":"2022-05-18T12:27:25.168191Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"# Make predictions with the model on hate dataset\nhate_predictions, hate_raw_outputs = model.predict(hate_list)\n\n# Attach predictions to test df for evaluation\nhate_df['predictions'] = hate_predictions\n\nprint(hate_df)","metadata":{"execution":{"iopub.status.busy":"2022-05-18T11:50:17.207785Z","iopub.execute_input":"2022-05-18T11:50:17.208649Z","iopub.status.idle":"2022-05-18T11:50:18.025458Z","shell.execute_reply.started":"2022-05-18T11:50:17.208613Z","shell.execute_reply":"2022-05-18T11:50:18.024633Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"# save hate result to html\nhate_df.to_html('/kaggle/working/hate_df.html')","metadata":{"execution":{"iopub.status.busy":"2022-05-18T11:50:52.698552Z","iopub.execute_input":"2022-05-18T11:50:52.699191Z","iopub.status.idle":"2022-05-18T11:50:52.715960Z","shell.execute_reply.started":"2022-05-18T11:50:52.699146Z","shell.execute_reply":"2022-05-18T11:50:52.715313Z"},"trusted":true},"execution_count":59,"outputs":[]}]}