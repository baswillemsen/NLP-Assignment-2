{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.\tTypos (6 points) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T10:46:09.814717Z",
     "iopub.status.busy": "2022-05-18T10:46:09.813128Z",
     "iopub.status.idle": "2022-05-18T10:46:25.593281Z",
     "shell.execute_reply": "2022-05-18T10:46:25.592431Z",
     "shell.execute_reply.started": "2022-05-18T10:46:09.814675Z"
    },
    "id": "UCvJT5jP297S"
   },
   "outputs": [],
   "source": [
    "# !pip install --user checklist\n",
    "# !pip install --user tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T10:47:14.906482Z",
     "iopub.status.busy": "2022-05-18T10:47:14.906194Z",
     "iopub.status.idle": "2022-05-18T10:47:14.916795Z",
     "shell.execute_reply": "2022-05-18T10:47:14.915939Z",
     "shell.execute_reply.started": "2022-05-18T10:47:14.906445Z"
    },
    "id": "1bHqRXL86MYl"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "import checklist\n",
    "from checklist.perturb import Perturb\n",
    "\n",
    "# reseed\n",
    "np.random.seed(42)\n",
    "\n",
    "# text\n",
    "cur_dir = os.getcwd()\n",
    "subset_test = pd.read_csv(cur_dir + '/input/olid-data/olid-subset-diagnostic-tests.csv')\n",
    "text = subset_test['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T10:47:15.629729Z",
     "iopub.status.busy": "2022-05-18T10:47:15.627129Z",
     "iopub.status.idle": "2022-05-18T10:47:15.638299Z",
     "shell.execute_reply": "2022-05-18T10:47:15.637514Z",
     "shell.execute_reply.started": "2022-05-18T10:47:15.629684Z"
    },
    "id": "hkOUFttSmVG7"
   },
   "outputs": [],
   "source": [
    "# overload existing method from checklist to add more than 1 typo\n",
    "def add_typos(string, typos=5):\n",
    "        \"\"\"Perturbation functions, swaps random characters with their neighbors\n",
    "        Parameters\n",
    "        ----------\n",
    "        string : str\n",
    "            input string\n",
    "        typos : int\n",
    "            number of typos to add\n",
    "        Returns\n",
    "        -------\n",
    "        list(string)\n",
    "            perturbed strings\n",
    "        \"\"\"\n",
    "        string = list(string)\n",
    "        swaps = np.random.choice(len(string) - 1, typos)\n",
    "        for swap in swaps:\n",
    "            tmp = string[swap]\n",
    "            string[swap] = string[swap + 1]\n",
    "            string[swap + 1] = tmp\n",
    "        return ''.join(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T10:47:44.963709Z",
     "iopub.status.busy": "2022-05-18T10:47:44.963387Z",
     "iopub.status.idle": "2022-05-18T10:47:44.984895Z",
     "shell.execute_reply": "2022-05-18T10:47:44.983944Z",
     "shell.execute_reply.started": "2022-05-18T10:47:44.963674Z"
    },
    "id": "W0_IIWKXAAyD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id                                               text  labels  \\\n",
      "0   89200      @USER @USER Who the hell does he think he is?       1   \n",
      "1   71294  #BREAKING. #Greece: Molotov cocktails fly afte...       1   \n",
      "2   55633  #OrrinHatch I can’t believe this sexist , clue...       1   \n",
      "3   16856  @USER @USER I'll use that one the next time im...       1   \n",
      "4   26072    0-1 lost my acca on the first fucking fight cba       1   \n",
      "..    ...                                                ...     ...   \n",
      "95  45518  @USER He is obviously getting suspended. He is...       0   \n",
      "96  51610  #Canada - EXCLUSIVE: #Trudeau #Liberals leave ...       0   \n",
      "97  26758  @USER @USER ...than why did you show us how ho...       0   \n",
      "98  30718  @USER @USER @USER You have yet to answer what ...       0   \n",
      "99  82558  #MAGA  ... got any ideas how she could have do...       0   \n",
      "\n",
      "                                           text_typos  \n",
      "0       @USER @SUER Wh othe hell doe she thinkh e si?  \n",
      "1   #BREAKING. #Greece: oMlotov cocktails fly afte...  \n",
      "2   #OrrinHatch I can’t believe this sexist , clue...  \n",
      "3   @USER @USER I'll use htat one the next time im...  \n",
      "4     0-1 lost my acca on the ifstr fucking figthc ba  \n",
      "..                                                ...  \n",
      "95  @USER He iso bviously getting suspneded. He is...  \n",
      "96  #Canada - ECXLUSIVE: #Trudeau #Liberals levae ...  \n",
      "97  U@ESR @USRE ...than why did you hsow ush ow ho...  \n",
      "98  @USER @USER@ USER You have yte to answer wath ...  \n",
      "99  #AMGA  ... got any ideas how seh could have dn...  \n",
      "\n",
      "[100 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# add_typos\n",
    "typos = Perturb.perturb(text, add_typos)\n",
    "#print(typos.data)\n",
    "\n",
    "# plug back into pandas|\n",
    "new_text = []\n",
    "for t in typos.data:\n",
    "    new_text.append(t[1])\n",
    "subset_test['text_typos'] = new_text\n",
    "print(subset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T10:47:45.759258Z",
     "iopub.status.busy": "2022-05-18T10:47:45.758438Z",
     "iopub.status.idle": "2022-05-18T10:47:47.523958Z",
     "shell.execute_reply": "2022-05-18T10:47:47.519583Z",
     "shell.execute_reply.started": "2022-05-18T10:47:45.759220Z"
    },
    "id": "lF47_HdaMuz0"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-3f2bb60d3b0e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# Make predictions with the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0msubset_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubset_raw_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset_test_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;31m#print(predictions, raw_outputs)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Preparing test data\n",
    "subset_test_df = subset_test[['text','labels']]\n",
    "subset_test_list = subset_test_df['text'].values.tolist()\n",
    "\n",
    "# Preparing typo test data\n",
    "subset_typos_test_df = subset_test[['text_typos','labels']]\n",
    "subset_typos_test_list = subset_typos_test_df['text_typos'].values.tolist()\n",
    "\n",
    "# using model trained previously\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)\n",
    "\n",
    "# Make predictions with the model\n",
    "subset_predictions, subset_raw_outputs = model.predict(subset_test_list)\n",
    "#print(predictions, raw_outputs)\n",
    "\n",
    "# Make predictions with the model on typos dataset\n",
    "subset_typos_predictions, subset_typos_raw_outputs = model.predict(subset_typos_test_list)\n",
    "#print(predictions, raw_outputs)\n",
    "\n",
    "# Attach predictions to test df for evaluation\n",
    "subset_test_df['predictions'] = subset_predictions\n",
    "\n",
    "# Attach predictions to typo test df for evaluation\n",
    "subset_typos_test_df['predictions'] = subset_typos_predictions\n",
    "\n",
    "# get class distributions for getting weighted F1 score later\n",
    "subset_freq_0 = subset_test_df['labels'].value_counts(normalize=True).iloc[0]\n",
    "subset_freq_1 = subset_test_df['labels'].value_counts(normalize=True).iloc[1]\n",
    "# note that class distributions are equal in this subset so weighted and macro\n",
    "# F1 scores will be the same\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "evaluation(subset_test_df, subset_freq_0, subset_freq_1)\n",
    "print('Correctly identified messages in data: ' + str(sum(subset_test_df['predictions'] == subset_test_df['labels'])))\n",
    "print('Total number of messages: ' + str(len(subset_test_df['predictions'])))\n",
    "\n",
    "# Calculate evaluation metrics for typos\n",
    "evaluation(subset_typos_test_df, subset_freq_0, subset_freq_1)\n",
    "print('Correctly identified messages in typo data: ' + str(sum(subset_typos_test_df['predictions'] == subset_typos_test_df['labels'])))\n",
    "print('Total number of messages: ' + str(len(subset_typos_test_df['predictions'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T10:52:59.254725Z",
     "iopub.status.busy": "2022-05-18T10:52:59.254299Z",
     "iopub.status.idle": "2022-05-18T10:52:59.300395Z",
     "shell.execute_reply": "2022-05-18T10:52:59.299606Z",
     "shell.execute_reply.started": "2022-05-18T10:52:59.254690Z"
    }
   },
   "outputs": [],
   "source": [
    "# save dataframes to html\n",
    "subset_test_df.to_html('/kaggle/working/subset_test_df.html')\n",
    "subset_typos_test_df.to_html('/kaggle/working/subset_typos_test_df.html')\n",
    "\n",
    "# read dataframes from html\n",
    "#subset_test_df = pd.read_html('../input/olid-subset-results/subset_test_df.html', index_col=0)[0]\n",
    "#subset_typos_test_df = pd.read_html('../input/olid-subset-results/subset_typos_test_df.html', index_col=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T11:01:44.917685Z",
     "iopub.status.busy": "2022-05-18T11:01:44.917230Z",
     "iopub.status.idle": "2022-05-18T11:01:44.923771Z",
     "shell.execute_reply": "2022-05-18T11:01:44.923042Z",
     "shell.execute_reply.started": "2022-05-18T11:01:44.917651Z"
    },
    "id": "h2lUSBmBeGy8"
   },
   "outputs": [],
   "source": [
    "print(subset_typos_test_df['text_typos'][4])\n",
    "print(subset_typos_test_df['text_typos'][16])\n",
    "print(subset_typos_test_df['text_typos'][29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T11:01:47.870524Z",
     "iopub.status.busy": "2022-05-18T11:01:47.869885Z",
     "iopub.status.idle": "2022-05-18T11:01:55.017081Z",
     "shell.execute_reply": "2022-05-18T11:01:55.016215Z",
     "shell.execute_reply.started": "2022-05-18T11:01:47.870484Z"
    }
   },
   "outputs": [],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T11:01:55.238436Z",
     "iopub.status.busy": "2022-05-18T11:01:55.238064Z",
     "iopub.status.idle": "2022-05-18T11:01:57.121367Z",
     "shell.execute_reply": "2022-05-18T11:01:57.120644Z",
     "shell.execute_reply.started": "2022-05-18T11:01:55.238396Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "pdata = list(nlp.pipe(text))\n",
    "\n",
    "#print(pdata)\n",
    "# reload data\n",
    "subset_test = pd.read_csv('../input/olid-data/olid-subset-diagnostic-tests.csv')\n",
    "text = subset_test['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T11:02:02.348157Z",
     "iopub.status.busy": "2022-05-18T11:02:02.347854Z",
     "iopub.status.idle": "2022-05-18T11:02:03.035790Z",
     "shell.execute_reply": "2022-05-18T11:02:03.034912Z",
     "shell.execute_reply.started": "2022-05-18T11:02:02.348115Z"
    },
    "id": "K0ID2jmbLtp6"
   },
   "outputs": [],
   "source": [
    "# add_negations\n",
    "negations = Perturb.perturb(pdata, Perturb.add_negation)\n",
    "#print(negations.data)\n",
    "#print(len(subset_test))\n",
    "#print(len(negations.data))\n",
    "\n",
    "ids = []\n",
    "text = []\n",
    "text_negations = []\n",
    "labels = []\n",
    "\n",
    "# build negations dataframe from scratch since the method uses spacy\n",
    "for n in negations.data:\n",
    "    for index, row in subset_test.iterrows():\n",
    "        if n[0] == row['text']:\n",
    "            ids.append(row['id'])\n",
    "            text.append(n[0])            \n",
    "            labels.append(row['labels'])\n",
    "            text_negations.append(n[1])\n",
    "\n",
    "# initialize data of lists.\n",
    "neg_data = {'id': ids, 'text': text,  'labels': labels, 'text_negations': text_negations}\n",
    "subset_test = pd.DataFrame(neg_data)\n",
    "print(subset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T11:02:25.020284Z",
     "iopub.status.busy": "2022-05-18T11:02:25.019552Z",
     "iopub.status.idle": "2022-05-18T11:02:26.730134Z",
     "shell.execute_reply": "2022-05-18T11:02:26.729270Z",
     "shell.execute_reply.started": "2022-05-18T11:02:25.020249Z"
    }
   },
   "outputs": [],
   "source": [
    "# Preparing test data\n",
    "subset_test_df = subset_test[['text','labels']]\n",
    "subset_test_list = subset_test_df['text'].values.tolist()\n",
    "\n",
    "# Preparing negations test data\n",
    "subset_negs_test_df = subset_test[['text_negations','labels']]\n",
    "subset_negs_test_list = subset_negs_test_df['text_negations'].values.tolist()\n",
    "\n",
    "# using model trained previously\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)\n",
    "\n",
    "# Make predictions with the model\n",
    "subset_predictions, subset_raw_outputs = model.predict(subset_test_list)\n",
    "#print(predictions, raw_outputs)\n",
    "\n",
    "# Make predictions with the model on negations dataset\n",
    "subset_negs_predictions, subset_negs_raw_outputs = model.predict(subset_negs_test_list)\n",
    "#print(predictions, raw_outputs)\n",
    "\n",
    "# Attach predictions to test df for evaluation\n",
    "subset_test_df['predictions'] = subset_predictions\n",
    "\n",
    "# Attach predictions to negations test df for evaluation\n",
    "subset_negs_test_df['predictions'] = subset_negs_predictions\n",
    "\n",
    "# get class distributions for getting weighted F1 score later\n",
    "subset_freq_0 = subset_test_df['labels'].value_counts(normalize=True).iloc[0]\n",
    "subset_freq_1 = subset_test_df['labels'].value_counts(normalize=True).iloc[1]\n",
    "# note that class distributions are equal in this subset so weighted and macro\n",
    "# F1 scores will be the same\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "evaluation(subset_test_df, subset_freq_0, subset_freq_1)\n",
    "print('Correctly identified messages in data: ' + str(sum(subset_test_df['predictions'] == subset_test_df['labels'])))\n",
    "print('Total number of messages: ' + str(len(subset_test_df['predictions'])))\n",
    "\n",
    "# Calculate evaluation metrics for negations\n",
    "evaluation(subset_negs_test_df, subset_freq_0, subset_freq_1)\n",
    "print('Correctly identified messages in negations data: ' + str(sum(subset_negs_test_df['predictions'] == subset_negs_test_df['labels'])))\n",
    "print('Total number of messages: ' + str(len(subset_negs_test_df['predictions'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T11:02:43.698260Z",
     "iopub.status.busy": "2022-05-18T11:02:43.697968Z",
     "iopub.status.idle": "2022-05-18T11:02:43.721259Z",
     "shell.execute_reply": "2022-05-18T11:02:43.720353Z",
     "shell.execute_reply.started": "2022-05-18T11:02:43.698229Z"
    }
   },
   "outputs": [],
   "source": [
    "# save dataframes to html\n",
    "#subset_test_df.to_html('/kaggle/working/subset_test_df.html')\n",
    "subset_negs_test_df.to_html('/kaggle/working/subset_negs_test_df.html')\n",
    "\n",
    "# read dataframes from html\n",
    "#subset_test_df = pd.read_html('../input/olid-subset-results/subset_test_df.html', index_col=0)[0]\n",
    "#subset_typos_test_df = pd.read_html('../input/olid-subset-results/subset_typos_test_df.html', index_col=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T11:33:09.227098Z",
     "iopub.status.busy": "2022-05-18T11:33:09.226424Z",
     "iopub.status.idle": "2022-05-18T11:33:09.553403Z",
     "shell.execute_reply": "2022-05-18T11:33:09.552636Z",
     "shell.execute_reply.started": "2022-05-18T11:33:09.227061Z"
    }
   },
   "outputs": [],
   "source": [
    "# question 7\n",
    "from checklist.editor import Editor\n",
    "\n",
    "editor = Editor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T12:27:25.107307Z",
     "iopub.status.busy": "2022-05-18T12:27:25.106511Z",
     "iopub.status.idle": "2022-05-18T12:27:25.169035Z",
     "shell.execute_reply": "2022-05-18T12:27:25.168191Z",
     "shell.execute_reply.started": "2022-05-18T12:27:25.107273Z"
    }
   },
   "outputs": [],
   "source": [
    "# getting masked language model suggestions\n",
    "hate = editor.suggest('I hate {mask}.')[:30]\n",
    "not_hate = editor.suggest('I don\\'t hate {mask}.')[:30]\n",
    "\n",
    "hate_mask = []\n",
    "for i in hate:\n",
    "    hate_mask.append('I hate ' + i)\n",
    "#print(hate_mask)\n",
    "\n",
    "not_hate_mask = []\n",
    "for i in hate:\n",
    "    not_hate_mask.append('I don\\'t hate ' + i)\n",
    "#print(not_hate_mask)\n",
    "\n",
    "# using the built-in lexicon, we also explore more specific mask subjects\n",
    "ret1 = editor.template('I hate {nationality}.')\n",
    "ret2 = editor.template('I don\\'t hate {nationality}.')\n",
    "ret3 = editor.template('I hate {religion}.')\n",
    "ret4 = editor.template('I don\\'t hate {religion}.')\n",
    "\n",
    "hate_nationalities = list(np.random.choice(ret1.data, 10))\n",
    "nationalities = list(np.random.choice(ret2.data, 10))\n",
    "hate_religions = list(np.random.choice(ret3.data, 10))\n",
    "religions = list(np.random.choice(ret4.data, 10))\n",
    "\n",
    "# build dataset\n",
    "hate_list = hate_mask + not_hate_mask + hate_nationalities + nationalities + hate_religions + religions\n",
    "hate_df = pd.DataFrame({'text': hate_list})\n",
    "print(hate_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T11:50:17.208649Z",
     "iopub.status.busy": "2022-05-18T11:50:17.207785Z",
     "iopub.status.idle": "2022-05-18T11:50:18.025458Z",
     "shell.execute_reply": "2022-05-18T11:50:18.024633Z",
     "shell.execute_reply.started": "2022-05-18T11:50:17.208613Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make predictions with the model on hate dataset\n",
    "hate_predictions, hate_raw_outputs = model.predict(hate_list)\n",
    "\n",
    "# Attach predictions to test df for evaluation\n",
    "hate_df['predictions'] = hate_predictions\n",
    "\n",
    "print(hate_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T11:50:52.699191Z",
     "iopub.status.busy": "2022-05-18T11:50:52.698552Z",
     "iopub.status.idle": "2022-05-18T11:50:52.715960Z",
     "shell.execute_reply": "2022-05-18T11:50:52.715313Z",
     "shell.execute_reply.started": "2022-05-18T11:50:52.699146Z"
    }
   },
   "outputs": [],
   "source": [
    "# save hate result to html\n",
    "hate_df.to_html('/kaggle/working/hate_df.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
